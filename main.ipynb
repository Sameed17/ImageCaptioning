{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228946d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os, pickle, torch, torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from train import Vocabulary, ImageCaptioner, greedy_search, load_captions, compute_bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_dir():\n",
    "    # Common Kaggle root\n",
    "\n",
    "    base_input = '/kaggle/input'\n",
    "    # Walk through the input directory to find where the images actually are\n",
    "    for root, dirs, files in os.walk(base_input):\n",
    "    # Look for the folder containing a high volume of jpg files\n",
    "        if len([f for f in files if f.endswith('.jpg')]) > 1000:\n",
    "            return root\n",
    "    return None\n",
    "IMAGE_DIR = find_image_dir()\n",
    "OUTPUT_FILE = 'flickr30k_features.pkl'\n",
    "if IMAGE_DIR:\n",
    "    print(f\" Found images at: {IMAGE_DIR}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find the Flickr30k image directory. Please ensure the dataset is added to the notebook.\")\n",
    "# --- THE DATASET CLASS ---\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.img_names = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img), name\n",
    "# --- REMAINDER OF THE PIPELINE (AS BEFORE) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model = nn.Sequential(*list(model.children())[:-1]) # Feature vector only\n",
    "model = nn.DataParallel(model).to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "dataset = FlickrDataset(IMAGE_DIR, transform)\n",
    "loader = DataLoader(dataset, batch_size=128, num_workers=4)\n",
    "features_dict = {}\n",
    "with torch.no_grad():\n",
    "    for imgs, names in tqdm(loader, desc=\"Extracting Features\"):\n",
    "        feats = model(imgs.to(device)).view(imgs.size(0), -1)\n",
    "        for i, name in enumerate(names):\n",
    "            features_dict[name] = feats[i].cpu().numpy()\n",
    "with open(OUTPUT_FILE, 'wb') as f:\n",
    "    pickle.dump(features_dict, f)\n",
    "print(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    PAD, START, END, UNK = \"<pad>\", \"<start>\", \"<end>\", \"<unk>\"\n",
    "\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.word2idx = {self.PAD: 0, self.START: 1, self.END: 2, self.UNK: 3}\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return text.lower().strip().split()\n",
    "\n",
    "    def build(self, captions):\n",
    "        count = Counter()\n",
    "        for cap in captions:\n",
    "            count.update(self._tokenize(cap))\n",
    "        for word, freq in count.items():\n",
    "            if freq >= self.min_freq and word not in self.word2idx:\n",
    "                i = len(self.word2idx)\n",
    "                self.word2idx[word] = i\n",
    "                self.idx2word[i] = word\n",
    "\n",
    "    def encode(self, caption, add_special=True):\n",
    "        out = [self.word2idx[self.START]] if add_special else []\n",
    "        for w in self._tokenize(caption):\n",
    "            out.append(self.word2idx.get(w, self.word2idx[self.UNK]))\n",
    "        if add_special:\n",
    "            out.append(self.word2idx[self.END])\n",
    "        return out\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \" \".join(self.idx2word.get(i, self.UNK) for i in ids if i not in {0, 1, 2})\n",
    "\n",
    "\n",
    "def load_captions(path):\n",
    "    pairs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines[1 if \"image\" in lines[0].lower() else 0:]:\n",
    "        if \",\" in line:\n",
    "            img, cap = line.strip().split(\",\", 1)\n",
    "            cap = cap.strip('\"').strip()\n",
    "            if cap:\n",
    "                pairs.append((img.strip(), cap))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden=512, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ImageCaptioner(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden=1024, embed=512, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(2048, hidden)\n",
    "        self.embed = nn.Embedding(vocab_size, embed, padding_idx=0)\n",
    "        self.embed_dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed, hidden, batch_first=True, dropout=0)\n",
    "        self.fc_dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        h = self.encoder(feats).unsqueeze(0)\n",
    "        c = h.new_zeros(h.size())\n",
    "        emb = self.embed_dropout(self.embed(caps[:, :-1]))\n",
    "        out, _ = self.lstm(emb, (h, c))\n",
    "        out = self.fc_dropout(out)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "def greedy_search(model, feat, vocab, max_len=50, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    feat = feat.to(device).unsqueeze(0)\n",
    "    h = model.encoder(feat).unsqueeze(0)\n",
    "    c = h.new_zeros(h.size())\n",
    "    tokens = [1]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - 1):\n",
    "            x = torch.tensor([[tokens[-1]]], dtype=torch.long, device=device)\n",
    "            emb = model.embed(x)\n",
    "            out, (h, c) = model.lstm(emb, (h, c))\n",
    "            logits = model.fc(out.squeeze(1))\n",
    "            next_id = logits.argmax(dim=-1).item()\n",
    "            tokens.append(next_id)\n",
    "            if next_id == 2:\n",
    "                break\n",
    "    return vocab.decode(tokens)\n",
    "\n",
    "\n",
    "def compute_bleu4(refs, hyps):\n",
    "    try:\n",
    "        from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "    except ImportError:\n",
    "        return 0.0\n",
    "    refs = [[r.split()] for r in refs]\n",
    "    hyps = [h.split() for h in hyps]\n",
    "    return corpus_bleu(refs, hyps, weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                      smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, feat_path, pairs, vocab, max_len=50):\n",
    "        with open(feat_path, \"rb\") as f:\n",
    "            self.feats = pickle.load(f)\n",
    "        self.pairs = [(img, cap) for img, cap in pairs if img in self.feats]\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img, cap = self.pairs[i]\n",
    "        ids = self.vocab.encode(cap)\n",
    "        if len(ids) > self.max_len:\n",
    "            ids = ids[: self.max_len - 1] + [2]\n",
    "        ids += [0] * (self.max_len - len(ids))\n",
    "        return torch.tensor(self.feats[img], dtype=torch.float32), torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    return torch.stack([b[0] for b in batch]), torch.stack([b[1] for b in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path=\"caption_model.pt\", device=\"cpu\"):\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}. Run train.py first.\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    vocab = checkpoint[\"vocab\"]\n",
    "    model = ImageCaptioner(len(vocab.word2idx)).to(device)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    model.eval()\n",
    "    history = checkpoint.get(\"history\", {})\n",
    "    return model, vocab, history\n",
    "\n",
    "\n",
    "def compute_precision_recall_f1(references, hypotheses):\n",
    "    precision_list, recall_list, f1_list = [], [], []\n",
    "    for reference, hypothesis in zip(references, hypotheses):\n",
    "        reference_words = set(reference.split())\n",
    "        hypothesis_words = set(hypothesis.split())\n",
    "        if not hypothesis_words:\n",
    "            precision_list.append(0.0)\n",
    "            recall_list.append(0.0)\n",
    "            f1_list.append(0.0)\n",
    "            continue\n",
    "        true_positives = len(reference_words & hypothesis_words)\n",
    "        precision = true_positives / len(hypothesis_words)\n",
    "        recall = true_positives / len(reference_words) if reference_words else 0.0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1_score)\n",
    "    return sum(precision_list) / len(precision_list), sum(recall_list) / len(recall_list), sum(f1_list) / len(f1_list)\n",
    "\n",
    "\n",
    "def caption_examples(model, vocab, features, pairs, images_dir, num_examples=5, device=\"cpu\"):\n",
    "    indices = list(range(len(pairs)))\n",
    "    random.shuffle(indices)\n",
    "    examples = []\n",
    "    for index in indices[:num_examples]:\n",
    "        image_name, ground_truth = pairs[index]\n",
    "        if image_name not in features:\n",
    "            continue\n",
    "        predicted_caption = greedy_search(model, torch.tensor(features[image_name]), vocab, device=device)\n",
    "        image_path = os.path.join(images_dir, image_name) if images_dir else None\n",
    "        if image_path and not os.path.isfile(image_path):\n",
    "            image_path = None\n",
    "        examples.append({\"image\": image_name, \"image_path\": image_path, \"ground_truth\": ground_truth, \"predicted\": predicted_caption})\n",
    "    return examples\n",
    "\n",
    "\n",
    "def plot_loss_curve(history, out_path=None):\n",
    "    if not history.get(\"train_loss\") and not history.get(\"val_loss\"):\n",
    "        print(\"No loss history. Run train.py first.\")\n",
    "        return\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print(\"matplotlib not installed.\")\n",
    "        return\n",
    "    output_path = out_path or \"loss_curve.png\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    if history.get(\"train_loss\"):\n",
    "        plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    if history.get(\"val_loss\"):\n",
    "        plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(output_path)\n",
    "    print(f\"Loss curve saved to {output_path}\")\n",
    "\n",
    "\n",
    "def run_evaluation():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, vocab, history = load_model(device=str(device))\n",
    "    pairs = load_captions(\"data/captions.txt\")\n",
    "    with open(\"flickr30k_features.pkl\", \"rb\") as features_file:\n",
    "        features = pickle.load(features_file)\n",
    "    pairs = [(image_name, caption) for image_name, caption in pairs if image_name in features]\n",
    "\n",
    "    print(\"\\n--- Caption Examples ---\")\n",
    "    examples = caption_examples(model, vocab, features, pairs, \"data/Images\", num_examples=5, device=device)\n",
    "    for example in examples:\n",
    "        print(f\"\\nImage: {example['image']}\")\n",
    "        print(f\"  Ground Truth: {example['ground_truth']}\")\n",
    "        print(f\"  Predicted:    {example['predicted']}\")\n",
    "\n",
    "    print(\"\\n--- Loss Curve ---\")\n",
    "    plot_loss_curve(history)\n",
    "\n",
    "    print(\"\\n--- Metrics ---\")\n",
    "    sample_indices = random.sample(range(len(pairs)), min(500, len(pairs)))\n",
    "    references = [pairs[index][1] for index in sample_indices]\n",
    "    hypotheses = [greedy_search(model, torch.tensor(features[pairs[index][0]]), vocab, device=device) for index in tqdm(sample_indices, desc=\"Evaluating\")]\n",
    "    bleu_score = compute_bleu4(references, hypotheses)\n",
    "    precision, recall, f1_score = compute_precision_recall_f1(references, hypotheses)\n",
    "    print(f\"BLEU-4:    {bleu_score:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1_score:.4f}\")\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        figure, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "        axes = axes.flatten()\n",
    "        for index, example in enumerate(examples[:5]):\n",
    "            axis = axes[index]\n",
    "            if example[\"image_path\"]:\n",
    "                image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "                axis.imshow(image)\n",
    "            axis.set_title(example[\"image\"][:20] + \"...\")\n",
    "            axis.axis(\"off\")\n",
    "            axis.text(0.5, -0.15, f\"GT: {example['ground_truth'][:50]}{'...' if len(example['ground_truth']) > 50 else ''}\\nPred: {example['predicted'][:50]}{'...' if len(example['predicted']) > 50 else ''}\", transform=axis.transAxes, fontsize=8, ha=\"center\", wrap=True)\n",
    "        axes[5].axis(\"off\")\n",
    "        plt.suptitle(\"Caption Examples\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"caption_examples.png\", bbox_inches=\"tight\")\n",
    "        print(\"\\nFigure saved to caption_examples.png\")\n",
    "    except Exception as error:\n",
    "        print(f\"Could not save figure: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pairs = load_captions(\"data/captions.txt\")\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build([cap for _, cap in pairs])\n",
    "\n",
    "ds = CaptionDataset(\"flickr30k_features.pkl\", pairs, vocab)\n",
    "train_ds, val_ds = random_split(ds, [int(0.9 * len(ds)), len(ds) - int(0.9 * len(ds))])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, collate_fn=collate_batch)\n",
    "\n",
    "model = ImageCaptioner(len(vocab.word2idx)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for feats, caps in tqdm(train_loader, leave=False):\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(feats, caps)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), caps[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    train_loss = total / len(train_loader)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for feats, caps in val_loader:\n",
    "            feats, caps = feats.to(device), caps.to(device)\n",
    "            logits = model(feats, caps)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), caps[:, 1:].reshape(-1))\n",
    "            total += loss.item()\n",
    "    val_loss = total / len(val_loader)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    print(f\"Epoch {epoch + 1} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "torch.save({\"model_state\": model.state_dict(), \"vocab\": vocab, \"history\": history}, \"caption_model.pt\")\n",
    "print(\"Saved to caption_model.pt\")\n",
    "\n",
    "model.eval()\n",
    "refs, hyps = [], []\n",
    "for idx in tqdm(val_ds.indices[: min(500, len(val_ds.indices))], desc=\"BLEU\"):\n",
    "    img, gt = val_ds.dataset.pairs[idx]\n",
    "    refs.append(gt)\n",
    "    hyps.append(greedy_search(model, torch.tensor(val_ds.dataset.feats[img]), vocab, device=device))\n",
    "print(f\"BLEU-4: {compute_bleu4(refs, hyps):.4f}\")\n",
    "\n",
    "run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
